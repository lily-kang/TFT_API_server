"""
sample_test_data.jsonì˜ ì‹¤ì œ ë°ì´í„°ë¡œ ë¶„ì„ê¸° API í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import os
import sys
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def load_sample_data():
    """sample_test_data.jsonì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
    try:
        with open('sample_test_data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data['test_cases'][0]['input']
    except Exception as e:
        print(f"âŒ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

async def test_sample_data():
    """ìƒ˜í”Œ ë°ì´í„°ë¡œ ì‹¤ì œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª sample_test_data.json ì‹¤ì œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*80)
    
    # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
    sample_input = load_sample_data()
    if not sample_input:
        return
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['OPENAI_API_KEY'] = 'test-key'
    os.environ['DEBUG'] = 'True'
    
    try:
        from core.analyzer import analyzer
        from core.metrics import metrics_extractor
        from core.judge import judge
        from models.request import MasterMetrics, ToleranceAbs, ToleranceRatio
        
        print("âœ… ëª¨ë“ˆ import ì„±ê³µ")
        
        # í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ë“¤
        test_texts = {
            "generated_passage": sample_input["generated_passage"]
        }
        
        # ë§ˆìŠ¤í„° ì§€í‘œì™€ í—ˆìš© ì˜¤ì°¨
        master = MasterMetrics(**sample_input["master"])
        tolerance_abs = ToleranceAbs(**sample_input["tolerance_abs"])
        tolerance_ratio = ToleranceRatio(**sample_input["tolerance_ratio"])
        
        print(f"\nğŸ¯ ë§ˆìŠ¤í„° ì§€í‘œ:")
        print(f"   - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {master.AVG_SENTENCE_LENGTH}")
        print(f"   - ë‚´í¬ì ˆ ë¹„ìœ¨: {master.All_Embedded_Clauses_Ratio}")
        print(f"   - A1A2 ì–´íœ˜ ë¹„ìœ¨: {master.CEFR_NVJD_A1A2_lemma_ratio}")
        
        print(f"\nğŸ“ í—ˆìš© ì˜¤ì°¨:")
        print(f"   - ì ˆëŒ€ê°’ ì˜¤ì°¨ (ë¬¸ì¥ê¸¸ì´): Â±{tolerance_abs.AVG_SENTENCE_LENGTH}")
        print(f"   - ë¹„ìœ¨ ì˜¤ì°¨ (ë‚´í¬ì ˆ): Â±{tolerance_ratio.All_Embedded_Clauses_Ratio}")
        print(f"   - ë¹„ìœ¨ ì˜¤ì°¨ (A1A2): Â±{tolerance_ratio.CEFR_NVJD_A1A2_lemma_ratio}")
        
        for text_name, text_content in test_texts.items():
            print(f"\n{'='*80}")
            print(f"ğŸ“ {text_name.upper()} ë¶„ì„ ì‹œì‘")
            print(f"{'='*80}")
            
            # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
            preview = text_content[:200] + "..." if len(text_content) > 200 else text_content
            print(f"ğŸ“– í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\n{preview}")
            
            print(f"\nğŸŒ ì‹¤ì œ ë¶„ì„ê¸° API í˜¸ì¶œ ì¤‘...")
            
            start_time = time.time()
            
            try:
                # ì‹¤ì œ ë¶„ì„ê¸° í˜¸ì¶œ
                result = await analyzer.analyze(text_content, include_syntax=True)
                
                end_time = time.time()
                duration = end_time - start_time
                
                print(f"âœ… ë¶„ì„ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ)")
                
                # ì§€í‘œ ì¶”ì¶œ (ìƒì„¸ ë¡œê¹… í¬í•¨)
                metrics = metrics_extractor.extract(result)
                
                # ì§€í‘œ í‰ê°€
                evaluation = judge.evaluate(metrics, master, tolerance_abs, tolerance_ratio)
                
                print(f"\nğŸ¯ {text_name} ë¶„ì„ ê²°ê³¼:")
                print(f"   ğŸ“ í‰ê·  ë¬¸ì¥ ê¸¸ì´: {metrics.AVG_SENTENCE_LENGTH:.3f} (ëª©í‘œ: {master.AVG_SENTENCE_LENGTH})")
                print(f"   ğŸ”— ë‚´í¬ì ˆ ë¹„ìœ¨: {metrics.All_Embedded_Clauses_Ratio:.3f} (ëª©í‘œ: {master.All_Embedded_Clauses_Ratio})")
                print(f"   ğŸ“š A1A2 ì–´íœ˜ ë¹„ìœ¨: {metrics.CEFR_NVJD_A1A2_lemma_ratio:.3f} (ëª©í‘œ: {master.CEFR_NVJD_A1A2_lemma_ratio})")
                
                print(f"\nâœ… í‰ê°€ ê²°ê³¼:")
                print(f"   ğŸ”§ êµ¬ë¬¸ í†µê³¼: {evaluation.syntax_pass}")
                print(f"   ğŸ“– ì–´íœ˜ í†µê³¼: {evaluation.lexical_pass}")
                
                # ìƒì„¸ í‰ê°€ ê²°ê³¼
                if hasattr(evaluation, 'details') and evaluation.details:
                    print(f"\nğŸ“Š ìƒì„¸ í‰ê°€:")
                    for metric_name, detail in evaluation.details.items():
                        if metric_name not in ["syntax_pass", "lexical_pass"]:
                            current_value = getattr(metrics, metric_name, None)
                            if current_value is not None:
                                min_val = detail.get("min_value", 0)
                                max_val = detail.get("max_value", 0)
                                is_pass = detail.get("is_pass", False)
                                status = "Pass" if is_pass else "Fail"
                                print(f"     {metric_name}: {current_value:.3f} vs [{min_val:.3f} ~ {max_val:.3f}] â†’ {status}")
                
            except Exception as e:
                end_time = time.time()
                duration = end_time - start_time
                print(f"âŒ {text_name} ë¶„ì„ ì‹¤íŒ¨ (ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ)")
                print(f"   ì˜¤ë¥˜: {str(e)}")
                
        print(f"\n{'='*80}")
        print("ğŸ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_sample_data()) 
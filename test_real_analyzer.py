"""
ì‹¤ì œ ì™¸ë¶€ ë¶„ì„ê¸° APIë¥¼ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸
Mock ì—†ì´ ì§„ì§œ ë¶„ì„ê¸° í˜¸ì¶œ
"""

import asyncio
import json
import os
import sys
from typing import Dict, Any
from unittest.mock import patch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# JSON íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
def load_test_data():
    """sample_test_data.jsonì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
    try:
        with open('sample_test_data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data['test_cases'][0]['input'], data['mock_responses']
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

# LLMë§Œ Mock (ë¶„ì„ê¸°ëŠ” ì‹¤ì œ í˜¸ì¶œ)
async def mock_llm_generate_multiple(prompt: str, temperatures: list) -> list:
    """Mock LLM ë‹¤ì¤‘ ìƒì„± (ë¶„ì„ê¸°ëŠ” ì‹¤ì œë¡œ í˜¸ì¶œë¨)"""
    test_data, mock_responses = load_test_data()
    candidates = mock_responses['llm_candidates']['syntax_fixed']
    
    print(f"\n=== ğŸ¤– LLM êµ¬ë¬¸ ìˆ˜ì • í”„ë¡¬í”„íŠ¸ (ê¸¸ì´: {len(prompt)} ê¸€ì) ===")
    print("(ì‹¤ì œ LLM ëŒ€ì‹  Mock ì‚¬ìš©)")
    
    print(f"\n=== ğŸ“ ìƒì„±ëœ í›„ë³´ ({len(temperatures)}ê°œ) ===")
    for i, (candidate, temp) in enumerate(zip(candidates, temperatures)):
        print(f"í›„ë³´ {i+1} (temp={temp}): {candidate[:100]}...")
    
    return candidates

async def mock_llm_select_best_candidate(selection_prompt: str, temperature: float = 0.1) -> int:
    """Mock LLM í›„ë³´ ì„ íƒ"""
    test_data, mock_responses = load_test_data()
    selection = mock_responses['llm_selection']
    
    print(f"\n=== ğŸ¯ LLM í›„ë³´ ì„ íƒ (Mock) ===")
    print(f"ì„ íƒ ê²°ê³¼: {selection}ë²ˆ")
    
    return int(selection)

async def test_with_real_analyzer():
    """ì‹¤ì œ ì™¸ë¶€ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ì‹¤ì œ ì™¸ë¶€ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_input, mock_responses = load_test_data()
    if not test_input:
        return
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['OPENAI_API_KEY'] = 'test-key-for-mock'
    os.environ['DEBUG'] = 'True'
    
    try:
        print("ğŸ“¦ ëª¨ë“ˆ import ì¤‘...")
        
        # ëª¨ë¸ import
        from models.request import PipelineItem, MasterMetrics, ToleranceAbs, ToleranceRatio
        from core.analyzer import analyzer
        
        print("âœ… ëª¨ë“ˆ import ì„±ê³µ")
        
        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ ì‹¤ì œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸
        test_text = "Iceland is beautiful. This country has waterfalls and hot springs."
        
        print(f"\nğŸŒ ì‹¤ì œ ì™¸ë¶€ ë¶„ì„ê¸° í˜¸ì¶œ ì¤‘...")
        print(f"- API URL: https://ils.jp.ngrok.io/api/enhanced_analyze")
        print(f"- í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸: {test_text}")
        print("- ì‘ë‹µ ëŒ€ê¸° ì¤‘... (ìˆ˜ ì´ˆ ì†Œìš”)")
        
        import time
        start_time = time.time()
        
        try:
            # ì‹¤ì œ ë¶„ì„ê¸° í˜¸ì¶œ
            result = await analyzer.analyze(test_text, include_syntax=True)
            
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"\nâœ… ì‹¤ì œ ë¶„ì„ê¸° ì‘ë‹µ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ)")
            print(f"ğŸ“Š ì‘ë‹µ êµ¬ì¡°:")
            
            # ì‘ë‹µ êµ¬ì¡° ì¶œë ¥
            if isinstance(result, dict):
                for key in result.keys():
                    if key == 'metrics' and isinstance(result[key], dict):
                        print(f"  - {key}: {{")
                        for metric_key, metric_value in result[key].items():
                            print(f"      {metric_key}: {metric_value}")
                        print("    }")
                    else:
                        value_preview = str(result[key])[:100]
                        print(f"  - {key}: {value_preview}...")
            
            # ì§€í‘œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            print(f"\nğŸ” ì§€í‘œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
            from core.metrics import metrics_extractor
            
            try:
                extracted_metrics = metrics_extractor.extract(result)
                print(f"âœ… ì§€í‘œ ì¶”ì¶œ ì„±ê³µ:")
                print(f"  - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {extracted_metrics.AVG_SENTENCE_LENGTH}")
                print(f"  - ë‚´í¬ì ˆ ë¹„ìœ¨: {extracted_metrics.All_Embedded_Clauses_Ratio}")
                print(f"  - A1A2 ì–´íœ˜ ë¹„ìœ¨: {extracted_metrics.CEFR_NVJD_A1A2_lemma_ratio}")
                
            except Exception as e:
                print(f"âŒ ì§€í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ core/metrics.pyì˜ extract í•¨ìˆ˜ë¥¼ ì‹¤ì œ API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”")
                
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            print(f"\nâŒ ì™¸ë¶€ ë¶„ì„ê¸° í˜¸ì¶œ ì‹¤íŒ¨ (ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ)")
            print(f"ì˜¤ë¥˜: {str(e)}")

            
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("=" * 70)
    print("ğŸŒ ì‹¤ì œ ì™¸ë¶€ ë¶„ì„ê¸° API í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    asyncio.run(test_with_real_analyzer()) 
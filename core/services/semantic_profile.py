import os
import json
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
import yaml
import re
from core.llm.client import llm_client_for_profile
from config.profile_gen_prompt import SEMANTIC_PROFILE_GEN_TEMPLATE, SUBTOPIC2_GEN_TEMPLATE
from utils.logging import logger


# Service-level constants
_PROJECT_ROOT = Path(__file__).resolve().parents[2]
_CONFIG_DIR = _PROJECT_ROOT / "config"
_OUTPUT_SCHEMA = _CONFIG_DIR / "output_schema.json"

# YAML ìºì‹œ (ì„±ëŠ¥ ìµœì í™”: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë°˜ë³µ ë¡œë”© ë°©ì§€)
_AR_CATEGORY_CACHE: Optional[Dict[str, List[str]]] = None

async def generate_semantic_profile_for_passage(passage_text: str) -> Dict[str, Any]:
	"""
	LLM 2ë‹¨ê³„ í˜¸ì¶œë¡œ sample í˜•íƒœì˜ ì˜ë¯¸ í”„ë¡œí•„ì„ ìƒì„±í•œë‹¤. (subtopic_2ëŠ” 2ì°¨ ìƒì„±)
	"""
	# 1) subtopic_2 ì œì™¸ í”„ë¡œí•„ ìƒì„±
	prompt_1 = SEMANTIC_PROFILE_GEN_TEMPLATE.format(var_passage_text=passage_text)
	
	# ğŸ“‹ 1ì°¨ í”„ë¡¬í”„íŠ¸ ë¡œê¹…
	logger.info("=" * 80)
	logger.info("ğŸ” [SEMANTIC PROFILE] 1ì°¨ í”„ë¡¬í”„íŠ¸ ìƒì„±")
	logger.info("=" * 80)
	logger.info(f"ğŸ“„ ì…ë ¥ ì§€ë¬¸ (ì²˜ìŒ 500ì):\n{passage_text[:500]}...")
	logger.info("-" * 80)
	logger.info(f"ğŸ“ 1ì°¨ í”„ë¡¬í”„íŠ¸:\n{prompt_1}")
	logger.info("=" * 80)
	# ------------------------------------------------------------
	first_pass_text = await llm_client_for_profile.generate_text(prompt_1, output_schema=_OUTPUT_SCHEMA)
	profile = _parse_first_pass_profile(first_pass_text)
	print("first_pass_text", first_pass_text)
	# print("profile", profile)
	# 2) subtopic_2 ìƒì„±ì„ ìœ„í•œ ìš”ì•½ + AR ì¹´í…Œê³ ë¦¬ í•„í„°
	summary_text = _summarize_for_subtopic2(profile)
	ar_map = _load_ar_category_map()
	sub1_title = profile.get("subtopic_1", "")
	relevant_items = ar_map.get(sub1_title, [])
	ar_subset_text = f"{sub1_title}: " + ", ".join(relevant_items) if relevant_items else sub1_title

	# 3) subtopic_2 ìƒì„±
	prompt_2 = SUBTOPIC2_GEN_TEMPLATE.format(
		var_passage_summary=summary_text,
		var_relevant_ar_category_data=ar_subset_text,
	)
	
	# ğŸ“‹ 2ì°¨ í”„ë¡¬í”„íŠ¸ ë¡œê¹…
	logger.info("=" * 80)
	logger.info("ğŸ” [SEMANTIC PROFILE] 2ì°¨ í”„ë¡¬í”„íŠ¸ ìƒì„±")
	logger.info("=" * 80)
	logger.info(f"ğŸ“Š 1ì°¨ í”„ë¡œí•„ ìš”ì•½:\n{summary_text}")
	logger.info("-" * 80)
	logger.info(f"ğŸ·ï¸  AR ì¹´í…Œê³ ë¦¬ ë°ì´í„°:\n{ar_subset_text}")
	logger.info("-" * 80)
	logger.info(f"ğŸ“ 2ì°¨ í”„ë¡¬í”„íŠ¸:\n{prompt_2}")
	logger.info("=" * 80)
	# ------------------------------------------------------------
	# print("prompt_2", prompt_2)
	subtopic_2 = (await llm_client_for_profile.generate_text(prompt_2)).strip()
	print("subtopic_2", subtopic_2)

	# 4) ê²°í•©
	profile["subtopic_2"] = subtopic_2
	
	# âœ… ìµœì¢… í”„ë¡œí•„ ë¡œê¹…
	logger.info("=" * 80)
	logger.info("âœ… [SEMANTIC PROFILE] ìƒì„± ì™„ë£Œ")
	logger.info("=" * 80)
	logger.info(f"ğŸ“‹ ìµœì¢… í”„ë¡œí•„:\n{json.dumps(profile, ensure_ascii=False, indent=2)}")
	logger.info("=" * 80)
	# ------------------------------------------------------------
	return profile


async def generate_semantic_profiles_batch(passages: List[str]) -> List[Dict[str, Any]]:
	"""
	ì—¬ëŸ¬ ì§€ë¬¸ì— ëŒ€í•´ ë³‘ë ¬ë¡œ ì˜ë¯¸ í”„ë¡œí•„ì„ ìƒì„±í•œë‹¤.
	"""
	tasks = [generate_semantic_profile_for_passage(p) for p in passages]
	results = await asyncio.gather(*tasks, return_exceptions=True)
	final: List[Dict[str, Any]] = []
	for res in results:
		if isinstance(res, Exception):
			final.append({
				"discipline": "",
				"subtopic_1": "",
				"subtopic_2": "",
				"central_focus": [],
				"key_concepts": [],
				"error": str(res),
			})
		else:
			final.append(res)
	return final


def _parse_first_pass_profile(text: str) -> Dict[str, Any]:
	"""
	ê°„ë‹¨í•œ ë¼ì¸ ê¸°ë°˜ íŒŒì„œë¥¼ í†µí•´ 1ì°¨ í”„ë¡œí•„ì„ ì¶”ì¶œí•œë‹¤.
	- ë‹¤ì–‘í•œ ë¼ë²¨ í‘œê¸° ë³€í˜• ì§€ì›: "**1) discipline:**", "1) discipline:", "Discipline:", ë“±
	- ë¦¬ìŠ¤íŠ¸ í•„ë“œ(central_focus, key_concepts)ì˜ ë©€í‹°ë¼ì¸ ë¶ˆë¦¿(-, *, â€¢, â€“) ì²˜ë¦¬
	- í…ìŠ¤íŠ¸ í•„ë“œì˜ ë©€í‹°ë¼ì¸ ë‚´ìš© ìˆ˜ì§‘
	"""

	profile: Dict[str, Any] = {
		"discipline": "",
		"subtopic_1": "",
		"central_focus": [],
		"key_concepts": [],
		"processes_structures": None,
		"setting_context": None,
		"purpose_objective": None,
		"genre_form": None,
	}

	# 1) ìš°ì„  JSON íŒŒì‹± ì‹œë„ (output_schema ê¸°ë°˜ êµ¬ì¡° ê°€ì •)
	try:
		parsed = json.loads(text)
		if isinstance(parsed, dict):
			# ìŠ¤í‚¤ë§ˆ í‚¤ ê·¸ëŒ€ë¡œ ë§¤í•‘, ëˆ„ë½ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
			profile["discipline"] = str(parsed.get("discipline", ""))
			profile["subtopic_1"] = str(parsed.get("subtopic_1", ""))
			cf = parsed.get("central_focus", []) or []
			kc = parsed.get("key_concepts", []) or []
			profile["central_focus"] = [str(x).strip() for x in cf if str(x).strip()]
			profile["key_concepts"] = [str(x).strip() for x in kc if str(x).strip()]
			# nullable í…ìŠ¤íŠ¸ í•„ë“œ ì²˜ë¦¬
			for k in ["processes_structures", "setting_context", "purpose_objective", "genre_form"]:
				v = parsed.get(k, None)
				if v is None:
					profile[k] = None
				else:
					profile[k] = str(v).strip() or None
			return profile
	except Exception:
		pass

	# 2) í…ìŠ¤íŠ¸(ë§ˆí¬ë‹¤ìš´) íŒŒì‹± í´ë°±: ë¼ë²¨ ë³€í˜•ì„ í‘œì¤€ í‚¤ë¡œ ë§¤í•‘
	label_variants: Dict[str, List[str]] = {
		"discipline": ["discipline"],
		"subtopic_1": ["subtopic_1", "subtopic 1", "subtopic-1"],
		"central_focus": ["central_focus", "central focus", "central-focus"],
		"key_concepts": ["key_concepts", "key concepts", "key-concepts"],
		"processes_structures": [
			"processes_structures",
			"processes/structures",
			"processes structures",
		],
		"setting_context": ["setting_context", "setting/context", "setting context"],
		"purpose_objective": ["purpose/objective", "purpose_objective", "purpose objective"],
		"genre_form": ["genre/form", "genre_form", "genre form"],
	}

	list_fields = {"central_focus", "key_concepts"}

	bullet_prefixes = ("-", "*", "â€¢", "â€“", "â€”")

	def sanitize_for_match(s: str) -> str:
		# ì†Œë¬¸ìí™”, ë§ˆí¬ë‹¤ìš´ *ì™€ ë°±í‹± ì œê±°, ì• ë²ˆí˜¸(1) ì œê±°
		s2 = s.strip().lower()
		s2 = s2.replace("`", "").replace("**", "").replace("*", "")
		# "1) ", "2)" íŒ¨í„´ ì œê±°
		s2 = re.sub(r"^\s*\d+\)\s*", "", s2)
		# êµµê²Œ í‘œê¸° í›„ ì½œë¡  ì•ë’¤ ê³µë°± ì •ë¦¬
		s2 = re.sub(r"\s+", " ", s2)
		return s2

	def find_label_start(line: str) -> Optional[str]:
		norm = sanitize_for_match(line)
		for key, variants in label_variants.items():
			for v in variants:
				if norm.startswith(f"{v}:"):
					return key
		return None

	def content_after_colon(raw_line: str) -> str:
		# ì²« ë²ˆì§¸ ì½œë¡  ë’¤ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë˜, ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ë¥¼ ì •ë¦¬
		parts = raw_line.split(":", 1)
		if len(parts) == 2:
			val = parts[1].strip()
			# ë¼ë²¨ì´ êµµê²Œ í‘œê¸°ëœ ê²½ìš° ë‚¨ì€ ** ì œê±°
			val = val.strip("* ")
			return val
		return ""

	def parse_inline_list(s: str) -> List[str]:
		if not s:
			return []
		if any(s.lstrip().startswith(p) for p in bullet_prefixes):
			# í•œ ì¤„ì— "- a - b" í˜•íƒœì¼ ìˆ˜ ìˆìŒ
			items = [t.strip() for t in s.split("-") if t.strip()]
			return items
		# ì½¤ë§ˆ êµ¬ë¶„ ë¦¬ìŠ¤íŠ¸
		return [t.strip() for t in s.split(",") if t.strip()]

	lines = [l.rstrip() for l in text.splitlines() if l.strip()]
	i = 0
	while i < len(lines):
		line = lines[i].strip()
		key = find_label_start(line)
		if not key:
			i += 1
			continue

		inline = content_after_colon(line)

		# ë¦¬ìŠ¤íŠ¸ í•„ë“œ ì²˜ë¦¬
		if key in list_fields:
			items: List[str] = []
			items.extend(parse_inline_list(inline))

			j = i + 1
			while j < len(lines):
				next_line = lines[j].strip()
				# ë‹¤ìŒ ë¼ë²¨ ì‹œì‘ì´ë©´ ì¤‘ë‹¨
				if find_label_start(next_line):
					break
				# ë¶ˆë¦¿ ë¼ì¸ ë˜ëŠ” ì¼ë°˜ ë¼ì¸ì—ì„œ í•­ëª© ì¶”ì¶œ
				if next_line.startswith(bullet_prefixes):
					items.append(next_line.lstrip("-*â€¢â€“â€” ").strip())
				else:
					# ë¶ˆë¦¿ì´ ì•„ë‹ˆì–´ë„ ì½¤ë§ˆ/ì„¸ë¯¸ì½œë¡  êµ¬ë¶„ì„ í—ˆìš©
					parts = re.split(r"[,;]\s*", next_line)
					for p in parts:
						p = p.strip()
						if p:
							items.append(p)
				j += 1

			# ì¤‘ë³µ/ê³µë°± ì •ë¦¬
			cleaned = []
			seen = set()
			for it in items:
				val = it.strip("- ")
				if val and val.lower() not in seen:
					seen.add(val.lower())
					cleaned.append(val)
			profile[key] = cleaned
			i = j
			continue

		# í…ìŠ¤íŠ¸ í•„ë“œ ì²˜ë¦¬ (ë©€í‹°ë¼ì¸)
		val_parts: List[str] = []
		if inline:
			val_parts.append(inline)
		j = i + 1
		while j < len(lines):
			next_line = lines[j].strip()
			if find_label_start(next_line):
				break
			# ë¶ˆë¦¿ìœ¼ë¡œ ì´ì–´ì ¸ë„ ë¬¸ì¥ ì¡°ê°ìœ¼ë¡œ í•©ì¹¨
			if next_line.startswith(bullet_prefixes):
				val_parts.append(next_line.lstrip("-*â€¢â€“â€” ").strip())
			else:
				val_parts.append(next_line)
			j += 1

		joined = " ".join([p for p in val_parts if p]).strip()
		profile[key] = joined if joined else None
		i = j

	return profile


def _summarize_for_subtopic2(profile: Dict[str, Any]) -> str:
	parts: List[str] = []
	parts.append(f"discipline: {profile.get('discipline','')}")
	parts.append(f"subtopic_1: {profile.get('subtopic_1','')}")
	central_focus = profile.get("central_focus", [])
	if central_focus:
		parts.append("central_focus: " + ", ".join(central_focus))
	key_concepts = profile.get("key_concepts", [])
	if key_concepts:
		parts.append("key_concepts: " + ", ".join(key_concepts))
	for k in ["processes_structures", "setting_context", "purpose_objective", "genre_form"]:
		v = profile.get(k)
		if v:
			parts.append(f"{k}: {v}")
	return "\n".join(parts)


def _load_ar_category_map() -> Dict[str, List[str]]:
	"""
	êµ¬ì¡°í™”ëœ YAML íŒŒì¼ì„ ë¡œë“œí•œë‹¤ (ìºì‹± ì ìš©).
	íŒŒì¼: config/ar_category_structured.yaml
	ìŠ¤í‚¤ë§ˆ: { "Subtopic_1": ["Option1", "Option2", ...], ... }

	ì„±ëŠ¥ ìµœì í™”: ì²« í˜¸ì¶œ ì‹œ ë¡œë“œ í›„ ìºì‹œì— ì €ì¥, ì´í›„ í˜¸ì¶œì€ ìºì‹œ ë°˜í™˜.
	ë°°ì¹˜ ì²˜ë¦¬ ì‹œ íŒŒì¼ I/O ì˜¤ë²„í—¤ë“œ ì œê±° (50ê°œ ë°°ì¹˜ â†’ 1ë²ˆë§Œ ë¡œë“œ).
	"""
	global _AR_CATEGORY_CACHE

	# ìºì‹œê°€ ì´ë¯¸ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
	if _AR_CATEGORY_CACHE is not None:
		return _AR_CATEGORY_CACHE

	# ìºì‹œ ì—†ìŒ - íŒŒì¼ ë¡œë“œ
	structured_yaml = _CONFIG_DIR / "ar_category_structured.yaml"
	if not structured_yaml.exists():
		logger.warning(f"AR category YAML íŒŒì¼ ì—†ìŒ: {structured_yaml}")
		_AR_CATEGORY_CACHE = {}
		return _AR_CATEGORY_CACHE

	try:
		with open(structured_yaml, "r", encoding="utf-8") as f:
			data = yaml.safe_load(f) or {}
			if isinstance(data, dict):
				_AR_CATEGORY_CACHE = {k: [str(x) for x in (v or [])] for k, v in data.items()}
			else:
				_AR_CATEGORY_CACHE = {}

		logger.info(f"AR category ë§µ ë¡œë“œ ì™„ë£Œ: {len(_AR_CATEGORY_CACHE)} ì¹´í…Œê³ ë¦¬ ìºì‹œë¨")
		return _AR_CATEGORY_CACHE
	except Exception as e:
		logger.error(f"AR category YAML ë¡œë“œ ì‹¤íŒ¨: {e}")
		_AR_CATEGORY_CACHE = {}
		return _AR_CATEGORY_CACHE


def clear_ar_category_cache():
	"""
	AR category ìºì‹œë¥¼ ë¬´íš¨í™”í•œë‹¤.
	YAML íŒŒì¼ ìˆ˜ì • í›„ ì¬ë¡œë“œê°€ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©.
	ì£¼ë¡œ ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ì‚¬ìš©.
	"""
	global _AR_CATEGORY_CACHE
	_AR_CATEGORY_CACHE = None
	logger.info("AR category ìºì‹œ ë¬´íš¨í™”ë¨") 
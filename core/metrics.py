from typing import Dict, Any
from models.internal import MetricsData
from utils.exceptions import MetricsExtractionError
from utils.logging import logger
import json


class MetricsExtractor:
    """ë¶„ì„ ê²°ê³¼ì—ì„œ ì§€í‘œë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def extract(self, raw_analysis: Dict[str, Any]) -> MetricsData:
        """
        ì™¸ë¶€ ë¶„ì„ê¸°ì˜ ì›ì‹œ ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì§€í‘œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            raw_analysis: ì™¸ë¶€ ë¶„ì„ê¸° API ì‘ë‹µ
            
        Returns:
            ì¶”ì¶œëœ ì§€í‘œ ë°ì´í„°
            
        Raises:
            MetricsExtractionError: ì§€í‘œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            logger.info("="*60)
            logger.info("ğŸ“Š ë¶„ì„ê¸° API ì‘ë‹µ ìƒì„¸ ë¡œê¹… ì‹œì‘")
            logger.info("="*60)
            
            # ì „ì²´ ì‘ë‹µ êµ¬ì¡° ë¡œê¹…
            # logger.info(f"ğŸ” ì „ì²´ ì‘ë‹µ í‚¤: {list(raw_analysis.keys())}")
            
            # ì‹¤ì œ API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
            data = raw_analysis.get("data", {})
            # logger.info(f"ğŸ“‹ data í‚¤: {list(data.keys())}")
            
            text_statistics = data.get("text_statistics", {})
            # logger.info(f"ğŸ“ˆ text_statistics í‚¤: {list(text_statistics.keys())}")
            
            # 1. í‰ê·  ë¬¸ì¥ ê¸¸ì´
            logger.info("\n" + "="*40)
            logger.info("ğŸ“ 1. í‰ê·  ë¬¸ì¥ ê¸¸ì´ ì¶”ì¶œ")
            logger.info("="*40)
            
            basic_overview = text_statistics.get("table_01_basic_overview", {})
            # logger.info(f"ğŸ” table_01_basic_overview ì „ì²´ ë‚´ìš©:")
            # logger.info(json.dumps(basic_overview, indent=2, ensure_ascii=False))
            
            avg_sentence_length = basic_overview.get("avg_sentence_length", 0.0)
            sentence_count = basic_overview.get("sentence_count", 1)
            logger.info(f"âœ… avg_sentence_length: {avg_sentence_length}")
            logger.info(f"âœ… sentence_count: {sentence_count}")
            
            # lexical_tokens ì¶”ì¶œ (t2 í…Œì´ë¸”ì—ì„œ)
            table_02 = text_statistics.get("table_02_detailed_tokens", {})
            lexical_tokens = table_02.get("lexical_tokens", 0)
            logger.info(f"âœ… lexical_tokens: {lexical_tokens}")
            
            # 2. ë‚´í¬ì ˆ ë¹„ìœ¨ ê³„ì‚°
            logger.info("\n" + "="*40)
            logger.info("ğŸ”— 2. ë‚´í¬ì ˆ ë¹„ìœ¨ ê³„ì‚°")
            logger.info("="*40)
            
            syntax_analysis = text_statistics.get("table_10_syntax_analysis", {})
            # logger.info(f"ğŸ” table_10_syntax_analysis ì „ì²´ ë‚´ìš©:")
            # logger.info(json.dumps(syntax_analysis, indent=2, ensure_ascii=False))
            
            adverbial_sentences = syntax_analysis.get("adverbial_clause_sentences", 0)
            coordinate_sentences = syntax_analysis.get("coordinate_clause_sentences", 0)
            nominal_sentences = syntax_analysis.get("nominal_clause_sentences", 0)
            relative_sentences = syntax_analysis.get("relative_clause_sentences", 0)
            
            # logger.info(f"ğŸ“Š adverbial_clause_sentences: {adverbial_sentences}")
            # logger.info(f"ğŸ“Š coordinate_clause_sentences: {coordinate_sentences}")
            # logger.info(f"ğŸ“Š nominal_clause_sentences: {nominal_sentences}")
            # logger.info(f"ğŸ“Š relative_clause_sentences: {relative_sentences}")
            
            total_clause_sentences = adverbial_sentences + coordinate_sentences + nominal_sentences + relative_sentences
            all_embedded_clauses_ratio = total_clause_sentences / sentence_count if sentence_count > 0 else 0.0
            
            logger.info(f"ğŸ“ˆ ì´ ì ˆ ë¬¸ì¥ ìˆ˜: {total_clause_sentences}")
            logger.info(f"ğŸ“ˆ ì „ì²´ ë¬¸ì¥ ìˆ˜: {sentence_count}")
            logger.info(f"âœ… All_Embedded_Clauses_Ratio: {all_embedded_clauses_ratio}")
            
            # 3. CEFR A1A2 ì–´íœ˜ ë¹„ìœ¨
            logger.info("\n" + "="*40)
            logger.info("ğŸ“š 3. CEFR_NVJD_lemma_A1A2 ì–´íœ˜ ë¹„ìœ¨")
            logger.info("="*40)
            
            lemma_metrics = text_statistics.get("table_11_lemma_metrics", {})
            # logger.info(f"ğŸ” table_11_lemma_metrics ì „ì²´ ë‚´ìš©:")
            # logger.info(json.dumps(lemma_metrics, indent=2, ensure_ascii=False))
            
            cefr_a1_ratio = lemma_metrics.get("cefr_a1_NVJD_lemma_ratio", 0.0)
            cefr_a2_ratio = lemma_metrics.get("cefr_a2_NVJD_lemma_ratio", 0.0)
            cefr_a1a2_ratio = cefr_a1_ratio + cefr_a2_ratio
            
            # logger.info(f"ğŸ“Š cefr_a1_NVJD_lemma_ratio: {cefr_a1_ratio}")
            # logger.info(f"ğŸ“Š cefr_a2_NVJD_lemma_ratio: {cefr_a2_ratio}")
            logger.info(f"âœ… CEFR_NVJD_A1A2_lemma_ratio: {cefr_a1a2_ratio}")
            
            # ìµœì¢… ê²°ê³¼
            extracted = MetricsData(
                AVG_SENTENCE_LENGTH=float(avg_sentence_length),
                All_Embedded_Clauses_Ratio=float(all_embedded_clauses_ratio),
                CEFR_NVJD_A1A2_lemma_ratio=float(cefr_a1a2_ratio)
            )
            
            logger.info("\n" + "="*60)
            logger.info("ğŸ¯ ìµœì¢… ì¶”ì¶œëœ ì§€í‘œ")
            logger.info("="*60)
            logger.info(f"âœ… AVG_SENTENCE_LENGTH: {extracted.AVG_SENTENCE_LENGTH}")
            logger.info(f"âœ… All_Embedded_Clauses_Ratio: {extracted.All_Embedded_Clauses_Ratio}")
            logger.info(f"âœ… CEFR_NVJD_A1A2_lemma_ratio: {extracted.CEFR_NVJD_A1A2_lemma_ratio}")
            logger.info("="*60)
            
            return extracted
            
        except Exception as e:
            logger.error(f"ì§€í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise MetricsExtractionError(f"ì§€í‘œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def format_detailed_result(self, metrics: MetricsData, evaluation_result: Dict[str, Dict]) -> str:
        """
        ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
        
        Args:
            metrics: ì¶”ì¶œëœ ì§€í‘œ ë°ì´í„°
            evaluation_result: í‰ê°€ ê²°ê³¼ (ì§€í‘œë³„ ìƒì„¸ ì •ë³´ í¬í•¨)
            
        Returns:
            í¬ë§·íŒ…ëœ ìƒì„¸ ê²°ê³¼ ë¬¸ìì—´
        """
        result_lines = []
        
        for metric_name, detail in evaluation_result.items():
            if metric_name in ["syntax_pass", "lexical_pass"]:
                continue
                
            current_value = getattr(metrics, metric_name, None)
            if current_value is None:
                continue
                
            min_val = detail.get("min_value", 0)
            max_val = detail.get("max_value", 0)
            is_pass = detail.get("is_pass", False)
            status = "Pass" if is_pass else "Fail"
            
            line = f"{metric_name}: {current_value:.3f} vs [{min_val:.3f} ~ {max_val:.3f}] â†’ {status}"
            result_lines.append(line)
        
        return "\n".join(result_lines)


# ì „ì—­ ì§€í‘œ ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤
metrics_extractor = MetricsExtractor() 
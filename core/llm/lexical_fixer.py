import json
import asyncio
import math
from typing import List, Tuple, Dict, Any, Optional
from core.llm.client import llm_client
from core.llm.prompt_builder import prompt_builder
from core.analyzer import analyzer
from core.metrics import metrics_extractor
from core.judge import judge
from config.settings import settings
from config.lexical_revision_prompt import Lexical_USER_INPUT_TEMPLATE, LEXICAL_FIXING_PROMPT_INCREASE, LEXICAL_FIXING_PROMPT_DECREASE
from models.request import MasterMetrics, ToleranceRatio
from models.internal import LLMCandidate, LLMResponse
from utils.exceptions import LLMAPIError
from utils.logging import logger


class LexicalFixer:
    """ì–´íœ˜ ìˆ˜ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.temperature = 0.2  # ì–´íœ˜ ìˆ˜ì •ì€ 0.2 ê³ ì •
        self.candidates_per_request = 3  # í›„ë³´ 3ê°œ ìƒì„±
    
    async def fix_lexical_with_params(
        self,
        text: str,
        master: MasterMetrics,
        tolerance_ratio: ToleranceRatio,
        current_cefr_ratio: float,
        direction: str = "increase",  # "increase" or "decrease"
        nvjd_total_lemma_count: Optional[int] = None,
        nvjd_a1a2_lemma_count: Optional[int] = None,
        cefr_breakdown: Optional[Dict[str, Any]] = None,
    ) -> Tuple[List[Dict], str, Dict, Any, int]:
        """
        ì–´íœ˜ ìˆ˜ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            text: ìˆ˜ì •í•  í…ìŠ¤íŠ¸
            master: ë§ˆìŠ¤í„° ì§€í‘œ
            tolerance_ratio: ë¹„ìœ¨ í—ˆìš© ì˜¤ì°¨
            current_cefr_ratio: í˜„ì¬ CEFR A1A2 ë¹„ìœ¨
            direction: "increase" (ì‰½ê²Œ) ë˜ëŠ” "decrease" (ì–´ë µê²Œ)
            
        Returns:
            (í›„ë³´ ìˆ˜ì •ì‚¬í•­ ë¦¬ìŠ¤íŠ¸, ì„ íƒëœ í…ìŠ¤íŠ¸, ìµœì¢… ì§€í‘œ, ìµœì¢… í‰ê°€, ìƒì„±ëœ í›„ë³´ ìˆ˜) íŠœí”Œ
        """
        try:
            logger.info(f"ì–´íœ˜ ìˆ˜ì • ì‹œì‘: {len(text)}ê¸€ì, ë°©í–¥={direction}")
            
            # ì–´íœ˜ ìˆ˜ì • íŒŒë¼ë¯¸í„° ê³„ì‚°
            # í˜„ì¬ CEFR ë¹„ìœ¨ê³¼ ëª©í‘œ ë²”ìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •í•  ë‹¨ì–´ ìˆ˜, ë°©í–¥, ì¼€ì´ìŠ¤ ë“±ì„ ê³„ì‚°
            lexical_params = prompt_builder.calculate_lexical_modification_count_nvjd(
                current_ratio=current_cefr_ratio,
                nvjd_total_lemma_count=nvjd_total_lemma_count,
                nvjd_a1a2_lemma_count=nvjd_a1a2_lemma_count,
                master=master,
                tolerance_ratio=tolerance_ratio,
            )
            
            # ê³„ì‚°ëœ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            num_modifications = int(lexical_params["num_modifications"])  # type: ignore
            # ë°©í–¥ ìš°ì„ ìˆœìœ„: ê³„ì‚°ëœ ë°©í–¥ â†’ í˜¸ì¶œì ì§€ì •ê°’
            direction = lexical_params.get("direction") if lexical_params.get("direction") and lexical_params.get("direction") != "none" else direction  # type: ignore
            target_lower = lexical_params["target_lower"]  # type: ignore
            target_upper = lexical_params["target_upper"]  # type: ignore
            case_label = lexical_params["case"]  # type: ignore (ì˜ˆ: "below_range", "within_range", "above_range")
            computed_current_ratio = current_cefr_ratio
            nvjd_total = nvjd_total_lemma_count
            nvjd_a1a2 = nvjd_a1a2_lemma_count
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (prompt_builder ì‚¬ìš©)
            prompt = prompt_builder.build_lexical_prompt(
                text=text,
                current_cefr_ratio=computed_current_ratio,
                target_min=target_lower,
                target_max=target_upper,
                num_modifications=num_modifications,
                direction=direction,
                cefr_breakdown=cefr_breakdown
            )
            
            # ğŸ“‹ ì–´íœ˜ ìˆ˜ì • í”„ë¡¬í”„íŠ¸ ë¡œê¹…
            logger.info("=" * 80)
            logger.info("ğŸ“š [LEXICAL FIX] í”„ë¡¬í”„íŠ¸ ìƒì„±")
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š í˜„ì¬ CEFR A1A2 ë¹„ìœ¨: {computed_current_ratio:.4f}")
            logger.info(f"ğŸ“Š ëª©í‘œ ë²”ìœ„: {target_lower:.4f} ~ {target_upper:.4f}")
            logger.info(f"ğŸ“Š NVJD ì´ ë ˜ë§ˆ: {nvjd_total}, A1A2 ë ˜ë§ˆ: {nvjd_a1a2}")
            logger.info(f"ğŸ¯ ìˆ˜ì • ë°©í–¥: {direction}, ìˆ˜ì • ë‹¨ì–´ ìˆ˜: {num_modifications}, Case: {case_label}")
            logger.info(f"ğŸŒ¡ï¸  Temperature: {self.temperature}")
            logger.info("-" * 80)
            logger.info(f"ğŸ¤– [SYSTEM í”„ë¡¬í”„íŠ¸]:\n{prompt[0]['content']}")
            logger.info("-" * 80)
            logger.info(f"ğŸ‘¤ [USER í”„ë¡¬í”„íŠ¸]:\n{prompt[1]['content']}")
            logger.info("=" * 80)
            
            # LLM í˜¸ì¶œ (temperature 0.2ë¡œ 3ê°œ í›„ë³´ ìƒì„±)
            llm_candidates = await self._generate_lexical_candidates(prompt)
            
            logger.info(f"LLMìœ¼ë¡œ {len(llm_candidates)}ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ")
            
            # í›„ë³´ íŒŒì‹± ë° í†µí•© sheet_data ìƒì„±
            parsed_candidates = []
            sheet_datas = []
            for i, cand_text in enumerate(llm_candidates, start=1):
                parsed = self._parse_lexical_candidate_output(cand_text)
                if parsed.get("parse_ok") and isinstance(parsed.get("sheet_data"), list):
                    sheet_datas.append(parsed["sheet_data"])
                parsed["index"] = i
                parsed_candidates.append(parsed)

            merged_sheet_data = self._merge_sheet_data(sheet_datas) if sheet_datas else []

            # í›„ë³´ ìš”ì•½(Revision Summaryë§Œ)ìœ¼ë¡œ ê²½ëŸ‰í™”
            candidate_summaries = [
                {"index": p.get("index"), "revision_summary": p.get("revision_summary")}
                for p in parsed_candidates
            ]

            return (
                [],  # modifications - ìœ ì§€
                text,  # selected_text - ìœ ì§€
                {  # metrics - ì–´íœ˜ í›„ë³´ íŒŒì‹± ê²°ê³¼ í¬í•¨
                    "NVJD_total_lemma_count": nvjd_total,
                    "NVJD_A1A2_lemma_count": nvjd_a1a2,
                    "CEFR_NVJD_A1A2_lemma_ratio": computed_current_ratio,
                    "target_lower": target_lower,
                    "target_upper": target_upper,
                    "case": case_label,
                    # í›„ë³´ë³„ ìƒì„¸ sheet_dataëŠ” ì œì™¸í•˜ê³  ìš”ì•½ë§Œ ì œê³µ
                    "lexical_candidates": candidate_summaries,
                    "lexical_sheet_data_merged": merged_sheet_data,
                },
                None,
                len(llm_candidates)
            )
            
        except Exception as e:
            logger.error(f"ì–´íœ˜ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}")
            raise LLMAPIError(f"ì–´íœ˜ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ì œê±°ë¨: _calculate_lexical_modifications_from_analysis (ì™¸ë¶€ ê³„ì‚° ì‚¬ìš©)

    def _extract_nvjd_counts(self, raw_analysis: Dict[str, Any]) -> Dict[str, int]:
        """ë¶„ì„ê¸° ì‘ë‹µì—ì„œ NVJD ê´€ë ¨ ì¹´ìš´íŠ¸ ì¶”ì¶œ
        
        Note: table_XXëŠ” ì™¸ë¶€ ë¶„ì„ê¸° APIì˜ ì‘ë‹µ í…Œì´ë¸” êµ¬ì¡°
        - table_02: ìƒì„¸ í† í° ì •ë³´
        - table_09: í’ˆì‚¬ ë¶„í¬
        - table_11: ë ˜ë§ˆ ì§€í‘œ
        """
        data = raw_analysis.get("data", {})
        text_statistics = data.get("text_statistics", {})
        table_02 = text_statistics.get("table_02_detailed_tokens", {})
        table_09 = text_statistics.get("table_09_pos_distribution", {})
        table_11 = text_statistics.get("table_11_lemma_metrics", {})

        counts = {
            "content_lemmas": int(table_02.get("content_lemmas", 0) or 0),
            "propn_lemma_count": int(table_09.get("propn_lemma_count", 0) or 0),
            "cefr_a1_NVJD_lemma_count": int(table_11.get("cefr_a1_NVJD_lemma_count", 0) or 0),
            "cefr_a2_NVJD_lemma_count": int(table_11.get("cefr_a2_NVJD_lemma_count", 0) or 0),
        }
        logger.info(f"NVJD ì¹´ìš´íŠ¸ ì¶”ì¶œ: {counts}")
        return counts

    def _parse_lexical_candidate_output(self, candidate_text: str) -> Dict[str, Any]:
        """lexical í›„ë³´ ì¶œë ¥ íŒŒì‹± (ë‘ í”„ë¡¬í”„íŠ¸ ë³€í˜• ëª¨ë‘ ì§€ì›)
        - ì„ í˜¸: { revision_summary, sheet_data: [ {st_id, original_sentence, corrections:[{original_clause,revised_clause,is_ok}]} ] }
        - ëŒ€ì•ˆ: [ { original_clause, revised_as, target_word_source_section, target_sentence_number } ]
        ë°˜í™˜: { parse_ok, revision_summary, sheet_data, error }
        """
        import re
        result: Dict[str, Any] = {"parse_ok": False, "revision_summary": None, "sheet_data": None, "error": None}
        try:
            # 1) JSON ì½”ë“œ íœìŠ¤ ìš°ì„  íƒì§€
            m = re.search(r"```json\s*(\{[\s\S]*?\}|\[[\s\S]*?\])\s*```", candidate_text)
            json_str = None
            if m:
                json_str = m.group(1)
            else:
                # ê°ì²´ ìš°ì„ 
                m2 = re.search(r"(\{[\s\S]*\})", candidate_text)
                if m2:
                    json_str = m2.group(1)
                else:
                    # ë°°ì—´ ëŒ€ì•ˆ
                    m3 = re.search(r"(\[[\s\S]*\])", candidate_text)
                    if m3:
                        json_str = m3.group(1)
            if not json_str:
                result["error"] = "no_json_found"
                return result

            data = json.loads(json_str)
            # ì¼€ì´ìŠ¤ A: ê°ì²´ with sheet_data
            if isinstance(data, dict) and "sheet_data" in data:
                sheet = data.get("sheet_data")
                if isinstance(sheet, list):
                    normalized = self._normalize_sheet_data(sheet)
                    result.update({
                        "parse_ok": True,
                        "revision_summary": data.get("revision_summary"),
                        "sheet_data": normalized
                    })
                    return result
                else:
                    result["error"] = "sheet_data_not_list"
                    return result
            # ì¼€ì´ìŠ¤ B: ë°°ì—´ of simple mods
            if isinstance(data, list):
                sheet = self._convert_flat_mods_to_sheet(data)
                result.update({"parse_ok": True, "sheet_data": sheet})
                return result
            result["error"] = "unexpected_json_shape"
            return result
        except Exception as e:
            result["error"] = str(e)
            return result

    def _normalize_sheet_data(self, sheet: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """sheet_data í•­ëª©ì„ í‘œì¤€ í˜•íƒœë¡œ ì •ë¦¬"""
        normalized: List[Dict[str, Any]] = []
        for row in sheet:
            st_id = row.get("st_id")
            orig = row.get("original_sentence")
            corrections = row.get("corrections", []) or []
            norm_corr = []
            for c in corrections:
                oc = c.get("original_clause")
                rc = c.get("revised_clause")
                is_ok = bool(c.get("is_ok", True))
                alts = c.get("alternatives") or []
                if not isinstance(alts, list):
                    alts = []
                if oc and rc:
                    norm_corr.append({
                        "original_clause": oc,
                        "revised_clause": rc,
                        "is_ok": is_ok,
                        "alternatives": alts, 
                    })
            if st_id is not None:
                normalized.append({
                    "st_id": int(st_id),
                    "original_sentence": orig,
                    "corrections": norm_corr
                })
        return normalized

    def _convert_flat_mods_to_sheet(self, arr: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """flat ë°°ì—´ í¬ë§·ì„ sheet_dataë¡œ ë³€í™˜"""
        by_st: Dict[int, Dict[str, Any]] = {}
        for item in arr:
            st = int(item.get("target_sentence_number", 0) or 0)
            oc = item.get("original_clause")
            rc = item.get("revised_as")
            if not st or not oc or not rc:
                continue
            row = by_st.setdefault(st, {"st_id": st, "original_sentence": None, "corrections": []})
            row["corrections"].append({
                "original_clause": oc,
                "revised_clause": rc,
                "is_ok": True
            })
        return sorted(by_st.values(), key=lambda r: r["st_id"])

    def _merge_sheet_data(self, sheet_datas: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ í›„ë³´ì˜ sheet_dataë¥¼ st_id ê¸°ì¤€ìœ¼ë¡œ ì·¨í•©í•˜ê³  ë™ì¼ corrections ì¤‘ë³µ ì œê±° (alternatives ë³‘í•© í¬í•¨)"""
        merged_by_st: Dict[int, Dict[str, Any]] = {}
        for sheet in sheet_datas:
            for row in sheet:
                st = int(row.get("st_id", 0) or 0)
                if not st:
                    continue
                target = merged_by_st.setdefault(st, {"st_id": st, "original_sentence": None, "corrections": []})
                # original_sentence ì±„ìš°ê¸° (ë¹„ì–´ìˆìœ¼ë©´ ìµœì´ˆ ê°’ ì‚¬ìš©)
                if not target.get("original_sentence") and row.get("original_sentence"):
                    target["original_sentence"] = row.get("original_sentence")
                # corrections ë³‘í•© (ì¤‘ë³µ ì œê±°, alternatives í¬í•¨)
                exist_map: Dict[tuple, Dict[str, Any]] = {
                    (c.get("original_clause"), c.get("revised_clause")): c for c in target["corrections"]
                }
                for c in (row.get("corrections", []) or []):
                    oc = c.get("original_clause")
                    rc = c.get("revised_clause")
                    if not oc or not rc:
                        continue
                    key = (oc, rc)
                    alts = c.get("alternatives") or []
                    if not isinstance(alts, list):
                        alts = []
                    if key in exist_map:
                        existing = exist_map[key]
                        existing_alts = existing.get("alternatives") or []
                        if not isinstance(existing_alts, list):
                            existing_alts = []
                        # union alternatives (string set)
                        existing["alternatives"] = list({*map(str, existing_alts), *map(str, alts)})
                        # combine is_ok conservatively
                        existing["is_ok"] = bool(existing.get("is_ok", True) and c.get("is_ok", True))
                    else:
                        new_item = {
                            "original_clause": oc,
                            "revised_clause": rc,
                            "is_ok": bool(c.get("is_ok", True)),
                            "alternatives": alts,
                        }
                        target["corrections"].append(new_item)
                        exist_map[key] = new_item
        # st_id ê¸°ì¤€ ì •ë ¬
        return sorted(merged_by_st.values(), key=lambda r: r["st_id"])
    
    async def _generate_lexical_candidates(self, prompt: List[Dict[str, str]]) -> List[str]:
        """ì–´íœ˜ ìˆ˜ì • í›„ë³´ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)"""
        # ë³‘ë ¬ë¡œ ëª¨ë“  í›„ë³´ ìƒì„± íƒœìŠ¤í¬ ìƒì„±
        tasks = [
            llm_client.generate_messages(prompt, temperature=self.temperature)
            for _ in range(self.candidates_per_request)
        ]

        logger.debug(f"ì–´íœ˜ í›„ë³´ {self.candidates_per_request}ê°œë¥¼ ë³‘ë ¬ë¡œ ìƒì„± ì‹œì‘...")

        # ë³‘ë ¬ ì‹¤í–‰ (ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨)
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì²˜ë¦¬
        candidates = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.warning(f"ì–´íœ˜ í›„ë³´ {i+1} ìƒì„± ì‹¤íŒ¨: {str(result)}")
            else:
                candidates.append(result)
                logger.debug(f"ì–´íœ˜ í›„ë³´ {i+1} ìƒì„± ì™„ë£Œ")

        logger.debug(f"ë³‘ë ¬ ìƒì„± ì™„ë£Œ: {len(candidates)}ê°œ ì„±ê³µ")
        return candidates


# ì „ì—­ ì–´íœ˜ ìˆ˜ì •ê¸° ì¸ìŠ¤í„´ìŠ¤
lexical_fixer = LexicalFixer() 
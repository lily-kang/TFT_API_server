from typing import List, Tuple, Dict, Any
import asyncio
from core.llm.client import llm_client
from core.llm.selector import CandidateSelector
from core.llm.prompt_builder import prompt_builder
from core.analyzer import analyzer
from core.metrics import metrics_extractor
from core.judge import judge
from config.settings import settings
from models.request import MasterMetrics, ToleranceAbs, ToleranceRatio
from models.internal import LLMCandidate, LLMResponse
from utils.exceptions import LLMAPIError
from utils.logging import logger


class SyntaxFixer:
    """êµ¬ë¬¸ ìˆ˜ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.selector = CandidateSelector()
        self.temperatures = settings.llm_temperatures  # [0.2, 0.3]
        self.candidates_per_temperature = settings.syntax_candidates_per_temperature  # 2
    

    async def fix_syntax_with_params(
        self,
        text: str,
        avg_target_min: float,
        avg_target_max: float,
        clause_target_min: float,
        clause_target_max: float,
        current_metrics: Dict[str, float],
        num_modifications: int,
        problematic_metric: str,
        referential_clauses: str = "",
        prompt_type: str = "decrease"
    ) -> Tuple[List[str], str, Any, Any, int]:
        """
        APIì—ì„œ ê³„ì‚°ëœ íŒŒë¼ë¯¸í„°ë¡œ êµ¬ë¬¸ ìˆ˜ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            text: ìˆ˜ì •í•  í…ìŠ¤íŠ¸
            master: ë§ˆìŠ¤í„° ì§€í‘œ
            tolerance_abs: ì ˆëŒ€ê°’ í—ˆìš© ì˜¤ì°¨
            tolerance_ratio: ë¹„ìœ¨ í—ˆìš© ì˜¤ì°¨
            current_metrics: í˜„ì¬ ì§€í‘œê°’ë“¤
            num_modifications: ìˆ˜ì •í•  ë¬¸ì¥ ìˆ˜ (APIì—ì„œ ìë™ ê³„ì‚°ë¨)
            problematic_metric: ë¬¸ì œê°€ ìˆëŠ” ì§€í‘œëª… (APIì—ì„œ ìë™ ê³„ì‚°ë¨)
            referential_clauses: ì°¸ì¡°ìš© ì ˆ ì •ë³´
            prompt_type: í”„ë¡¬í”„íŠ¸ íƒ€ì… ("increase" ë˜ëŠ” "decrease")
            
        Returns:
            (í›„ë³´ ë¦¬ìŠ¤íŠ¸, ì„ íƒëœ í…ìŠ¤íŠ¸, ìµœì¢… ì§€í‘œ, ìµœì¢… í‰ê°€, ì „ì²´ ìƒì„±ëœ í›„ë³´ ìˆ˜) íŠœí”Œ
            
        Raises:
            LLMAPIError: LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            logger.info(f"êµ¬ë¬¸ ìˆ˜ì • ì‹œì‘ (API ê³„ì‚°ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©): {len(text)} ê¸€ì")
            logger.info(f"Temperature ì„¤ì •: {self.temperatures}, ê° temperatureë³„ {self.candidates_per_temperature}ê°œ í›„ë³´")
            logger.info(f"API ê³„ì‚° ê²°ê³¼ - ë¬¸ì œì§€í‘œ: {problematic_metric}, ìˆ˜ì •ìˆ˜: {num_modifications}, í”„ë¡¬í”„íŠ¸íƒ€ì…: {prompt_type}")
            
            # current_metrics í‚¤ ì´ë¦„ ë§¤í•‘
            mapped_metrics = {
                'avg_sentence_length': current_metrics.get('AVG_SENTENCE_LENGTH', 0),
                'all_embedded_clauses_ratio': current_metrics.get('All_Embedded_Clauses_Ratio', 0)
            }
                        
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (APIì—ì„œ ê³„ì‚°ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
            prompt = prompt_builder.build_syntax_prompt(
                text, avg_target_min, avg_target_max, clause_target_min, clause_target_max,
                mapped_metrics,
                problematic_metric, num_modifications, referential_clauses, prompt_type
            )
            
            # ğŸ“‹ êµ¬ë¬¸ ìˆ˜ì • í”„ë¡¬í”„íŠ¸ ë¡œê¹…
            logger.info("=" * 80)
            logger.info("ğŸ”§ [SYNTAX FIX] í”„ë¡¬í”„íŠ¸ ìƒì„±")
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š ëª©í‘œ ë²”ìœ„:")
            logger.info(f"   - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {avg_target_min:.2f} ~ {avg_target_max:.2f}")
            logger.info(f"   - ë‚´í¬ì ˆ ë¹„ìœ¨: {clause_target_min:.3f} ~ {clause_target_max:.3f}")
            logger.info(f"ğŸ¯ ë¬¸ì œ ì§€í‘œ: {problematic_metric}, ìˆ˜ì • ë¬¸ì¥ ìˆ˜: {num_modifications}, íƒ€ì…: {prompt_type}")
            logger.info("-" * 80)
            logger.info(f"ğŸ¤– [SYSTEM í”„ë¡¬í”„íŠ¸]:\n{prompt[0]['content']}")
            logger.info("-" * 80)
            logger.info(f"ğŸ‘¤ [USER í”„ë¡¬í”„íŠ¸]:\n{prompt[1]['content']}")
            logger.info("=" * 80)
            
            # ê° temperatureë³„ë¡œ ì—¬ëŸ¬ í›„ë³´ ìƒì„±
            candidates = await llm_client.generate_multiple_messages_per_temperature(prompt)
            
            total_candidates = len(self.temperatures) * self.candidates_per_temperature
            logger.info(f"LLMìœ¼ë¡œ ì´ {len(candidates)}ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ (ì˜ˆìƒ: {total_candidates}ê°œ)")
            
            # ìƒì„±ëœ í›„ë³´ë“¤ì˜ í…ìŠ¤íŠ¸ ë‚´ìš© í™•ì¸ (ë””ë²„ê¹…ìš©)
            for i, candidate in enumerate(candidates):
                temp_index = i // self.candidates_per_temperature
                candidate_index_in_temp = (i % self.candidates_per_temperature) + 1
                temp_value = self.temperatures[temp_index] if temp_index < len(self.temperatures) else "Unknown"
                logger.info(f"=== í›„ë³´ {i+1} (temp={temp_value}, {candidate_index_in_temp}/{self.candidates_per_temperature}) ===")
                logger.info(f"ê¸¸ì´: {len(candidate)}ê¸€ì")
                logger.info(f"ì²˜ìŒ 100ê¸€ì: {candidate[:100]}...")
                logger.info(f"ë§ˆì§€ë§‰ 100ê¸€ì: ...{candidate[-100:]}")
                logger.info("=" * 60)
            
            # ê° í›„ë³´ë¥¼ ë¶„ì„ê¸°ë¡œ ê²€ì¦ (ë³‘ë ¬ ì²˜ë¦¬)
            logger.info(f"ì´ {len(candidates)}ê°œ í›„ë³´ë¥¼ ë³‘ë ¬ë¡œ ë¶„ì„ ì‹œì‘...")
            
            # ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±
            analysis_tasks = []
            candidate_info = []
            
            for i, candidate in enumerate(candidates):
                # Temperatureë³„ ì •ë³´ ê³„ì‚°
                temp_index = i // self.candidates_per_temperature
                candidate_index_in_temp = (i % self.candidates_per_temperature) + 1
                temp_value = self.temperatures[temp_index] if temp_index < len(self.temperatures) else "Unknown"
                
                # ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±
                task = self._analyze_candidate_with_ranges(candidate, avg_target_min, avg_target_max, clause_target_min, clause_target_max)
                analysis_tasks.append(task)
                candidate_info.append({
                    'index': i + 1,
                    'text': candidate,
                    'temperature': temp_value,
                    'temp_candidate_num': candidate_index_in_temp
                })
            
            # ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
            try:
                analysis_results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
                logger.info(f"ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: ì´ {len(analysis_results)}ê°œ ê²°ê³¼")
                
                # ê²°ê³¼ ì²˜ë¦¬
                valid_candidates = []
                for i, (result, info) in enumerate(zip(analysis_results, candidate_info)):
                    if isinstance(result, Exception):
                        logger.warning(f"í›„ë³´ {info['index']} ë¶„ì„ ì‹¤íŒ¨: {str(result)}")
                        continue
                    
                    candidate_metrics, candidate_evaluation = result
                    
                    # êµ¬ë¬¸ ì§€í‘œ í†µê³¼ ì—¬ë¶€ í™•ì¸
                    if candidate_evaluation.syntax_pass == "PASS":
                        valid_candidates.append({
                            'text': info['text'],
                            'index': info['index'],
                            'temperature': info['temperature'],
                            'temp_candidate_num': info['temp_candidate_num'],
                            'metrics': candidate_metrics,
                            'evaluation': candidate_evaluation
                        })
                        logger.info(f"í›„ë³´ {info['index']}: êµ¬ë¬¸ ì§€í‘œ í†µê³¼ âœ… (temp={info['temperature']})")
                        logger.info(f"   - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {candidate_metrics.get('AVG_SENTENCE_LENGTH', 0):.3f}")
                        logger.info(f"   - ë‚´í¬ì ˆ ë¹„ìœ¨: {candidate_metrics.get('All_Embedded_Clauses_Ratio', 0):.3f}")
                    else:
                        # ëª©í‘œ ë²”ìœ„ ê³„ì‚° (ë¡œê·¸ìš©)
                        length_min = avg_target_min
                        length_max = avg_target_max
                        clause_min = clause_target_min
                        clause_max = clause_target_max
                        
                        logger.info(f"í›„ë³´ {info['index']}: êµ¬ë¬¸ ì§€í‘œ ì‹¤íŒ¨ âŒ (temp={info['temperature']})")
                        logger.info(f"   - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {candidate_metrics.get('AVG_SENTENCE_LENGTH', 0):.3f} (ëª©í‘œ: {length_min:.2f}-{length_max:.2f})")
                        logger.info(f"   - ë‚´í¬ì ˆ ë¹„ìœ¨: {candidate_metrics.get('All_Embedded_Clauses_Ratio', 0):.3f} (ëª©í‘œ: {clause_min:.3f}-{clause_max:.3f})")
                        
                        # ì–´ë–¤ ì§€í‘œê°€ ì‹¤íŒ¨í–ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ í‘œì‹œ
                        length_pass = length_min <= candidate_metrics.get('AVG_SENTENCE_LENGTH', 0) <= length_max
                        clause_pass = clause_min <= candidate_metrics.get('All_Embedded_Clauses_Ratio', 0) <= clause_max
                        logger.info(f"   - ë¬¸ì¥ê¸¸ì´ í†µê³¼: {'âœ…' if length_pass else 'âŒ'}, ë‚´í¬ì ˆ í†µê³¼: {'âœ…' if clause_pass else 'âŒ'}")
                
            except Exception as e:
                logger.error(f"ë³‘ë ¬ ë¶„ì„ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
                # í´ë°±: ìˆœì°¨ ì²˜ë¦¬
                logger.info("í´ë°±: ìˆœì°¨ ì²˜ë¦¬ë¡œ ì¬ì‹œë„...")
                valid_candidates = await self._analyze_candidates_sequential(candidates, avg_target_min, avg_target_max, clause_target_min, clause_target_max)
            
            # í†µê³¼í•œ í›„ë³´ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨
            if not valid_candidates:
                logger.warning("ëª¨ë“  í›„ë³´ê°€ êµ¬ë¬¸ ì§€í‘œë¥¼ í†µê³¼í•˜ì§€ ëª»í•¨")
                raise LLMAPIError("ìƒì„±ëœ ëª¨ë“  í›„ë³´ê°€ êµ¬ë¬¸ ì§€í‘œ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            logger.info(f"{len(valid_candidates)}ê°œ í›„ë³´ê°€ êµ¬ë¬¸ ì§€í‘œ í†µê³¼")
            
            # í†µê³¼í•œ í›„ë³´ë“¤ ì¤‘ì—ì„œ ìµœì  ì„ íƒ
            if len(valid_candidates) == 1:
                selected_candidate = valid_candidates[0]
                logger.info(f"í›„ë³´ {selected_candidate['index']}ë²ˆë§Œ í†µê³¼í•˜ì—¬ ìë™ ì„ íƒ (temp={selected_candidate['temperature']})")
            else:
                # ì—¬ëŸ¬ í›„ë³´ ì¤‘ LLMì´ ì„ íƒ
                candidate_texts = [item['text'] for item in valid_candidates]
                selected_text = await self.selector.select_best(candidate_texts)
                
                # ì„ íƒëœ í…ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” í›„ë³´ ì°¾ê¸°
                selected_candidate = None
                for candidate in valid_candidates:
                    if candidate['text'] == selected_text:
                        selected_candidate = candidate
                        break
                
                if not selected_candidate:
                    # ì„ íƒ ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ í†µê³¼ í›„ë³´ ì‚¬ìš©
                    selected_candidate = valid_candidates[0]
                    logger.warning("ì„ íƒ ì‹¤íŒ¨ë¡œ ì²« ë²ˆì§¸ í†µê³¼ í›„ë³´ ì‚¬ìš©")
                
                logger.info(f"LLMì´ í›„ë³´ {selected_candidate['index']}ë²ˆ ì„ íƒ (temp={selected_candidate['temperature']})")
            
            # ì„ íƒëœ í›„ë³´ì˜ ìƒì„¸ ì§€í‘œ ë¡œê¹…
            selected_metrics = selected_candidate['metrics']
            logger.info(f"ì„ íƒëœ í›„ë³´ ì§€í‘œ: í‰ê· ë¬¸ì¥ê¸¸ì´={selected_metrics.get('AVG_SENTENCE_LENGTH', 0):.2f}, "
                       f"ë‚´í¬ì ˆë¹„ìœ¨={selected_metrics.get('All_Embedded_Clauses_Ratio', 0):.3f}")
            
            # ëª¨ë“  í›„ë³´ì™€ ì„ íƒëœ í…ìŠ¤íŠ¸ ë°˜í™˜
            all_candidate_texts = [item['text'] for item in valid_candidates]
            selected_text = selected_candidate['text']
            selected_metrics = selected_candidate['metrics']
            selected_evaluation = selected_candidate['evaluation']
            
            # ì „ì²´ ìƒì„±ëœ í›„ë³´ ìˆ˜ ê³„ì‚°
            total_candidates_generated = len(self.temperatures) * self.candidates_per_temperature
            
            logger.info(f"êµ¬ë¬¸ ìˆ˜ì • ì™„ë£Œ: {total_candidates_generated}ê°œ ìƒì„± â†’ {len(valid_candidates)}ê°œ í†µê³¼ â†’ 1ê°œ ì„ íƒ (ë¬¸ì œ ì§€í‘œ: {problematic_metric})")
            return all_candidate_texts, selected_text, selected_metrics, selected_evaluation, total_candidates_generated
            
        except Exception as e:
            logger.error(f"êµ¬ë¬¸ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}")
            raise LLMAPIError(f"êµ¬ë¬¸ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}")
    
    # async def fix_syntax(
    #     self,
    #     text: str,
    #     master: MasterMetrics,
    #     tolerance_abs: ToleranceAbs,
    #     tolerance_ratio: ToleranceRatio,
    #     current_metrics: Dict[str, float],
    #     referential_clauses: str = "",
    #     n_candidates: int = 4  # ê¸°ë³¸ê°’: 2 temperatures Ã— 2 candidates = 4
    # ) -> Tuple[List[str], str]:
    #     """
    #     êµ¬ë¬¸ ìˆ˜ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
    #     Args:
    #         text: ìˆ˜ì •í•  í…ìŠ¤íŠ¸
    #         master: ë§ˆìŠ¤í„° ì§€í‘œ
    #         tolerance_abs: ì ˆëŒ€ê°’ í—ˆìš© ì˜¤ì°¨
    #         tolerance_ratio: ë¹„ìœ¨ í—ˆìš© ì˜¤ì°¨
    #         current_metrics: í˜„ì¬ ì§€í‘œê°’ë“¤
    #         referential_clauses: ì°¸ì¡°ìš© ì ˆ ì •ë³´
    #         n_candidates: ìƒì„±í•  í›„ë³´ ê°œìˆ˜ (ë¬´ì‹œë¨ - temperature ì„¤ì • ìš°ì„ )
            
    #     Returns:
    #         (í›„ë³´ ë¦¬ìŠ¤íŠ¸, ì„ íƒëœ í…ìŠ¤íŠ¸) íŠœí”Œ
            
    #     Raises:
    #         LLMAPIError: LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
    #     """
    #     try:
    #         logger.info(f"êµ¬ë¬¸ ìˆ˜ì • ì‹œì‘: {len(text)} ê¸€ì")
    #         logger.info(f"Temperature ì„¤ì •: {self.temperatures}, ê° temperatureë³„ {self.candidates_per_temperature}ê°œ í›„ë³´")
            
    #         # current_metrics í‚¤ ì´ë¦„ ë§¤í•‘
    #         mapped_metrics = {
    #             'avg_sentence_length': current_metrics.get('AVG_SENTENCE_LENGTH', 0),
    #             'embedded_clauses_ratio': current_metrics.get('All_Embedded_Clauses_Ratio', 0)
    #         }
            
    #         # ë¬¸ì œ ì§€í‘œ ê²°ì •
    #         problematic_metric = prompt_builder.determine_problematic_metric(
    #             mapped_metrics, master, tolerance_abs, tolerance_ratio
    #         )
            
    #         if not problematic_metric:
    #             logger.info("ë¬¸ì œê°€ ìˆëŠ” ì§€í‘œê°€ ì—†ì–´ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜")
    #             return [text], text
            
    #         # ìˆ˜ì • ë¬¸ì¥ ìˆ˜ëŠ” API ìš”ì²­ì—ì„œ ì „ë‹¬ë°›ìœ¼ë¯€ë¡œ ê³ ì •ê°’ 3 ì‚¬ìš©
    #         num_modifications = 3
            
    #         # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    #         prompt = prompt_builder.build_syntax_prompt(
    #             text, master, tolerance_abs, tolerance_ratio,
    #             mapped_metrics, problematic_metric, num_modifications, referential_clauses
    #         )
    #         print(prompt)
    #         # ê° temperatureë³„ë¡œ ì—¬ëŸ¬ í›„ë³´ ìƒì„±
    #         candidates = await llm_client.generate_multiple_per_temperature(
    #             prompt, 
    #             self.temperatures, 
    #             self.candidates_per_temperature
    #         )
            
    #         total_candidates = len(self.temperatures) * self.candidates_per_temperature
    #         logger.info(f"LLMìœ¼ë¡œ ì´ {len(candidates)}ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ (ì˜ˆìƒ: {total_candidates}ê°œ)")
            
    #         # ê° í›„ë³´ë¥¼ ë¶„ì„ê¸°ë¡œ ê²€ì¦ (ë³‘ë ¬ ì²˜ë¦¬)
    #         logger.info(f"ì´ {len(candidates)}ê°œ í›„ë³´ë¥¼ ë³‘ë ¬ë¡œ ë¶„ì„ ì‹œì‘...")
            
    #         # ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±
    #         analysis_tasks = []
    #         candidate_info = []
            
    #         for i, candidate in enumerate(candidates):
    #             # Temperatureë³„ ì •ë³´ ê³„ì‚°
    #             temp_index = i // self.candidates_per_temperature
    #             candidate_index_in_temp = (i % self.candidates_per_temperature) + 1
    #             temp_value = self.temperatures[temp_index] if temp_index < len(self.temperatures) else "Unknown"
                
    #             # ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±
    #             task = self._analyze_candidate(candidate, master, tolerance_abs, tolerance_ratio)
    #             analysis_tasks.append(task)
    #             candidate_info.append({
    #                 'index': i + 1,
    #                 'text': candidate,
    #                 'temperature': temp_value,
    #                 'temp_candidate_num': candidate_index_in_temp
    #             })
            
    #         # ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
    #         try:
    #             analysis_results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
    #             logger.info(f"ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: ì´ {len(analysis_results)}ê°œ ê²°ê³¼")
                
    #             # ê²°ê³¼ ì²˜ë¦¬
    #             valid_candidates = []
    #             for i, (result, info) in enumerate(zip(analysis_results, candidate_info)):
    #                 if isinstance(result, Exception):
    #                     logger.warning(f"í›„ë³´ {info['index']} ë¶„ì„ ì‹¤íŒ¨: {str(result)}")
    #                     continue
                    
    #                 candidate_metrics, candidate_evaluation = result
                    
    #                 # êµ¬ë¬¸ ì§€í‘œ í†µê³¼ ì—¬ë¶€ í™•ì¸
    #                 if candidate_evaluation.syntax_pass == "PASS":
    #                     valid_candidates.append({
    #                         'text': info['text'],
    #                         'index': info['index'],
    #                         'temperature': info['temperature'],
    #                         'temp_candidate_num': info['temp_candidate_num'],
    #                         'metrics': candidate_metrics,
    #                         'evaluation': candidate_evaluation
    #                     })
    #                     logger.info(f"í›„ë³´ {info['index']}: êµ¬ë¬¸ ì§€í‘œ í†µê³¼ âœ… (temp={info['temperature']})")
    #                     logger.info(f"   - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {candidate_metrics.AVG_SENTENCE_LENGTH:.3f}")
    #                     logger.info(f"   - ë‚´í¬ì ˆ ë¹„ìœ¨: {candidate_metrics.All_Embedded_Clauses_Ratio:.3f}")
    #                 else:
    #                     # ëª©í‘œ ë²”ìœ„ ê³„ì‚° (ë¡œê·¸ìš©)
    #                     length_min = master.AVG_SENTENCE_LENGTH - tolerance_abs.AVG_SENTENCE_LENGTH
    #                     length_max = master.AVG_SENTENCE_LENGTH + tolerance_abs.AVG_SENTENCE_LENGTH
    #                     clause_tolerance = master.All_Embedded_Clauses_Ratio * tolerance_ratio.All_Embedded_Clauses_Ratio
    #                     clause_min = master.All_Embedded_Clauses_Ratio - clause_tolerance
    #                     clause_max = master.All_Embedded_Clauses_Ratio + clause_tolerance
                        
    #                     logger.info(f"í›„ë³´ {info['index']}: êµ¬ë¬¸ ì§€í‘œ ì‹¤íŒ¨ âŒ (temp={info['temperature']})")
    #                     logger.info(f"   - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {candidate_metrics.AVG_SENTENCE_LENGTH:.3f} (ëª©í‘œ: {length_min:.2f}-{length_max:.2f})")
    #                     logger.info(f"   - ë‚´í¬ì ˆ ë¹„ìœ¨: {candidate_metrics.All_Embedded_Clauses_Ratio:.3f} (ëª©í‘œ: {clause_min:.3f}-{clause_max:.3f})")
                        
    #                     # ì–´ë–¤ ì§€í‘œê°€ ì‹¤íŒ¨í–ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ í‘œì‹œ
    #                     length_pass = length_min <= candidate_metrics.AVG_SENTENCE_LENGTH <= length_max
    #                     clause_pass = clause_min <= candidate_metrics.All_Embedded_Clauses_Ratio <= clause_max
    #                     logger.info(f"   - ë¬¸ì¥ê¸¸ì´ í†µê³¼: {'âœ…' if length_pass else 'âŒ'}, ë‚´í¬ì ˆ í†µê³¼: {'âœ…' if clause_pass else 'âŒ'}")
                
    #         except Exception as e:
    #             logger.error(f"ë³‘ë ¬ ë¶„ì„ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
    #             # í´ë°±: ìˆœì°¨ ì²˜ë¦¬
    #             logger.info("í´ë°±: ìˆœì°¨ ì²˜ë¦¬ë¡œ ì¬ì‹œë„...")
    #             valid_candidates = await self._analyze_candidates_sequential(candidates, master, tolerance_abs, tolerance_ratio)
            
    #         # í†µê³¼í•œ í›„ë³´ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨
    #         if not valid_candidates:
    #             logger.warning("ëª¨ë“  í›„ë³´ê°€ êµ¬ë¬¸ ì§€í‘œë¥¼ í†µê³¼í•˜ì§€ ëª»í•¨")
    #             raise LLMAPIError("ìƒì„±ëœ ëª¨ë“  í›„ë³´ê°€ êµ¬ë¬¸ ì§€í‘œ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
    #         logger.info(f"{len(valid_candidates)}ê°œ í›„ë³´ê°€ êµ¬ë¬¸ ì§€í‘œ í†µê³¼")
            
    #         # í†µê³¼í•œ í›„ë³´ë“¤ ì¤‘ì—ì„œ ìµœì  ì„ íƒ
    #         if len(valid_candidates) == 1:
    #             selected_candidate = valid_candidates[0]
    #             logger.info(f"í›„ë³´ {selected_candidate['index']}ë²ˆë§Œ í†µê³¼í•˜ì—¬ ìë™ ì„ íƒ (temp={selected_candidate['temperature']})")
    #         else:
    #             # ì—¬ëŸ¬ í›„ë³´ ì¤‘ LLMì´ ì„ íƒ
    #             candidate_texts = [item['text'] for item in valid_candidates]
    #             selected_text = await self.selector.select_best(candidate_texts)
                
    #             # ì„ íƒëœ í…ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” í›„ë³´ ì°¾ê¸°
    #             selected_candidate = None
    #             for candidate in valid_candidates:
    #                 if candidate['text'] == selected_text:
    #                     selected_candidate = candidate
    #                     break
                
    #             if not selected_candidate:
    #                 # ì„ íƒ ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ í†µê³¼ í›„ë³´ ì‚¬ìš©
    #                 selected_candidate = valid_candidates[0]
    #                 logger.warning("ì„ íƒ ì‹¤íŒ¨ë¡œ ì²« ë²ˆì§¸ í†µê³¼ í›„ë³´ ì‚¬ìš©")
                
    #             logger.info(f"LLMì´ í›„ë³´ {selected_candidate['index']}ë²ˆ ì„ íƒ (temp={selected_candidate['temperature']})")
            
    #         # ì„ íƒëœ í›„ë³´ì˜ ìƒì„¸ ì§€í‘œ ë¡œê¹…
    #         selected_metrics = selected_candidate['metrics']
    #         logger.info(f"ì„ íƒëœ í›„ë³´ ì§€í‘œ: í‰ê· ë¬¸ì¥ê¸¸ì´={selected_metrics.get('AVG_SENTENCE_LENGTH', 0):.2f}, "
    #                    f"ë‚´í¬ì ˆë¹„ìœ¨={selected_metrics.get('All_Embedded_Clauses_Ratio', 0):.3f}")
            
    #         # ëª¨ë“  í›„ë³´ì™€ ì„ íƒëœ í…ìŠ¤íŠ¸ ë°˜í™˜
    #         all_candidate_texts = [item['text'] for item in valid_candidates]
    #         selected_text = selected_candidate['text']
    #         selected_metrics = selected_candidate['metrics']
    #         selected_evaluation = selected_candidate['evaluation']
            
    #         logger.info(f"êµ¬ë¬¸ ìˆ˜ì • ì™„ë£Œ: {len(candidates)}ê°œ ìƒì„± â†’ {len(valid_candidates)}ê°œ í†µê³¼ â†’ 1ê°œ ì„ íƒ (ë¬¸ì œ ì§€í‘œ: {problematic_metric})")
    #         return all_candidate_texts, selected_text, selected_metrics, selected_evaluation
            
    #     except Exception as e:
    #         logger.error(f"êµ¬ë¬¸ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}")
    #         raise LLMAPIError(f"êµ¬ë¬¸ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    async def _analyze_candidate(self, candidate: str, master: MasterMetrics, tolerance_abs: ToleranceAbs, tolerance_ratio: ToleranceRatio) -> Tuple[Dict[str, float], Dict[str, str]]:
        """
        ë‹¨ì¼ í›„ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ì§€í‘œì™€ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            candidate: ë¶„ì„í•  í…ìŠ¤íŠ¸
            master: ë§ˆìŠ¤í„° ì§€í‘œ
            tolerance_abs: ì ˆëŒ€ê°’ í—ˆìš© ì˜¤ì°¨
            tolerance_ratio: ë¹„ìœ¨ í—ˆìš© ì˜¤ì°¨
            
        Returns:
            (ì§€í‘œ ë”•ì…”ë„ˆë¦¬, í‰ê°€ ê²°ê³¼) íŠœí”Œ
        """
        try:
            # í›„ë³´ í…ìŠ¤íŠ¸ ë¶„ì„
            raw_analysis = await analyzer.analyze(candidate, include_syntax=True)
            candidate_metrics_obj = metrics_extractor.extract(raw_analysis)
            candidate_evaluation = judge.evaluate(candidate_metrics_obj, master, tolerance_abs, tolerance_ratio)
            return candidate_metrics_obj.model_dump(), candidate_evaluation
        except Exception as e:
            logger.error(f"í›„ë³´ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise

    async def _analyze_candidate_with_ranges(
        self, 
        candidate: str, 
        avg_target_min: float,
        avg_target_max: float, 
        clause_target_min: float,
        clause_target_max: float
    ) -> Tuple[Dict[str, float], Any]:
        """
        ê°œë³„ ë²”ìœ„ ê°’ìœ¼ë¡œ í›„ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ì§€í‘œì™€ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            candidate: ë¶„ì„í•  í…ìŠ¤íŠ¸
            avg_target_min: í‰ê·  ë¬¸ì¥ ê¸¸ì´ ëª©í‘œ ìµœì†Œê°’
            avg_target_max: í‰ê·  ë¬¸ì¥ ê¸¸ì´ ëª©í‘œ ìµœëŒ€ê°’
            clause_target_min: ë‚´í¬ì ˆ ë¹„ìœ¨ ëª©í‘œ ìµœì†Œê°’
            clause_target_max: ë‚´í¬ì ˆ ë¹„ìœ¨ ëª©í‘œ ìµœëŒ€ê°’
            
        Returns:
            (ì§€í‘œ ë”•ì…”ë„ˆë¦¬, í‰ê°€ ê²°ê³¼) íŠœí”Œ
        """
        try:
            # í›„ë³´ í…ìŠ¤íŠ¸ ë¶„ì„
            raw_analysis = await analyzer.analyze(candidate, include_syntax=True)
            candidate_metrics_obj = metrics_extractor.extract(raw_analysis)
            candidate_metrics_dict = candidate_metrics_obj.model_dump()
            candidate_evaluation = judge.evaluate_with_ranges(
                candidate_metrics_dict, avg_target_min, avg_target_max,
                clause_target_min, clause_target_max
            )
            return candidate_metrics_dict, candidate_evaluation
        except Exception as e:
            logger.error(f"í›„ë³´ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise

    async def _analyze_candidates_sequential(self, candidates: List[str], avg_target_min: float, avg_target_max: float, clause_target_min: float, clause_target_max: float) -> List[Dict[str, Any]]:
        """
        ìˆœì°¨ì ìœ¼ë¡œ í›„ë³´ë¥¼ ë¶„ì„í•˜ì—¬ í†µê³¼í•œ í›„ë³´ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            candidates: ë¶„ì„í•  í›„ë³´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            avg_target_min: í‰ê·  ë¬¸ì¥ ê¸¸ì´ ëª©í‘œ ìµœì†Œê°’
            avg_target_max: í‰ê·  ë¬¸ì¥ ê¸¸ì´ ëª©í‘œ ìµœëŒ€ê°’
            clause_target_min: ë‚´í¬ì ˆ ë¹„ìœ¨ ëª©í‘œ ìµœì†Œê°’
            clause_target_max: ë‚´í¬ì ˆ ë¹„ìœ¨ ëª©í‘œ ìµœëŒ€ê°’
            
        Returns:
            í†µê³¼í•œ í›„ë³´ë“¤ì˜ ì •ë³´ë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸
        """
        valid_candidates = []
        for i, candidate in enumerate(candidates):
            try:
                # Temperatureë³„ ì •ë³´ ê³„ì‚°
                temp_index = i // self.candidates_per_temperature
                candidate_index_in_temp = (i % self.candidates_per_temperature) + 1
                temp_value = self.temperatures[temp_index] if temp_index < len(self.temperatures) else "Unknown"
                
                logger.info(f"í›„ë³´ {i+1} ë¶„ì„ ì¤‘... (temp={temp_value}, {candidate_index_in_temp}/{self.candidates_per_temperature})")
                
                # í›„ë³´ í…ìŠ¤íŠ¸ ë¶„ì„
                raw_analysis = await analyzer.analyze(candidate, include_syntax=True)
                candidate_metrics = metrics_extractor.extract(raw_analysis)
                candidate_evaluation = judge.evaluate_with_ranges(
                    candidate_metrics, avg_target_min, avg_target_max, 
                    clause_target_min, clause_target_max
                )
                
                # êµ¬ë¬¸ ì§€í‘œ í†µê³¼ ì—¬ë¶€ í™•ì¸
                if candidate_evaluation.syntax_pass == "PASS":
                    valid_candidates.append({
                        'text': candidate,
                        'index': i + 1,
                        'temperature': temp_value,
                        'temp_candidate_num': candidate_index_in_temp,
                        'metrics': candidate_metrics,
                        'evaluation': candidate_evaluation
                    })
                    logger.info(f"í›„ë³´ {i+1}: êµ¬ë¬¸ ì§€í‘œ í†µê³¼ âœ… (temp={temp_value})")
                    logger.info(f"   - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {candidate_metrics.get('AVG_SENTENCE_LENGTH', 0):.3f}")
                    logger.info(f"   - ë‚´í¬ì ˆ ë¹„ìœ¨: {candidate_metrics.get('All_Embedded_Clauses_Ratio', 0):.3f}")
                else:
                    # ëª©í‘œ ë²”ìœ„ ê³„ì‚° (ë¡œê·¸ìš©)
                    length_min = avg_target_min
                    length_max = avg_target_max
                    clause_min = clause_target_min
                    clause_max = clause_target_max
                    
                    logger.info(f"í›„ë³´ {i+1}: êµ¬ë¬¸ ì§€í‘œ ì‹¤íŒ¨ âŒ (temp={temp_value})")
                    logger.info(f"   - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {candidate_metrics.get('AVG_SENTENCE_LENGTH', 0):.3f} (ëª©í‘œ: {length_min:.2f}-{length_max:.2f})")
                    logger.info(f"   - ë‚´í¬ì ˆ ë¹„ìœ¨: {candidate_metrics.get('All_Embedded_Clauses_Ratio', 0):.3f} (ëª©í‘œ: {clause_min:.3f}-{clause_max:.3f})")
                    
                    # ì–´ë–¤ ì§€í‘œê°€ ì‹¤íŒ¨í–ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ í‘œì‹œ
                    length_pass = length_min <= candidate_metrics.get('AVG_SENTENCE_LENGTH', 0) <= length_max
                    clause_pass = clause_min <= candidate_metrics.get('All_Embedded_Clauses_Ratio', 0) <= clause_max
                    logger.info(f"   - ë¬¸ì¥ê¸¸ì´ í†µê³¼: {'âœ…' if length_pass else 'âŒ'}, ë‚´í¬ì ˆ í†µê³¼: {'âœ…' if clause_pass else 'âŒ'}")
                    
            except Exception as e:
                logger.warning(f"í›„ë³´ {i+1} ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                continue
        return valid_candidates


# ì „ì—­ êµ¬ë¬¸ ìˆ˜ì •ê¸° ì¸ìŠ¤í„´ìŠ¤
syntax_fixer = SyntaxFixer() 